# RAM
## DRAM(Dynamic Random Access Memory)

![Untitled](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/15dcae3c-798c-47f2-bce5-aadd25e1f5a4)

우리가 컴퓨터를 구매할때 램이 8기가다, 16기가다 이런 말을 할 때, 나오는 그 램이 바로 이 메인 메모리인 DRAM입니다. DRAM은 컴퓨터에서 데이터를 일시적으로 저장하고 처리하는 데 사용되는 주기억 장치입니다. DRAM은 컴퓨터의 실행 속도와 성능에 중요한 역할을 합니다.

위와 같이 메모리에는 한 줄에 8비트씩 명령어가 저장됩니다. 1101011 이런게 어떻게 단순한 숫자가 아니라 “명령어” 인지는 cpu를 공부할때 알아보도록 합시다. 위와 같이 32비트 즉 4줄의 명령어를 한번에 처리 할 수 있으면 x32비트 프로세서라하고, 64비트 즉 8줄의 명령어를 한번에 처리할 수 있다면 x64비트 프로세서라 합니다.

RAM은 "Random Access Memory"의 약자로,  데이터에 랜덤한 접근이 가능하다는 특징을 갖고 있습니다. 즉, 컴퓨터가 저장된 데이터에 어떤 순서로 접근하든지 빠른 속도로 접근할 수 있습니다. 위의 그림에서 맨 위에있는 10001101에 접근하는 속도와 맨 밑의 있는 11110000에 접근하는 속도가 차이가 없다는 소리입니다. 어떻게 이게 가능할까요?

![Untitled 1](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/721fe7fd-ff32-4389-85eb-8c3048d64fb2)

그건 바로 RAM에 있는 데이터들이 각자의 주소를 가지고 있기 때문입니다. 위 그림에서 한 줄을 셀(cell) 이라고 하는데 위처럼 100,101,102… 이렇게 메모리 셀마다 자신의 주소를 가지고 있고, 각 셀에 저장된 데이터를 읽거나 쓸 수 있는 방식을 의미합니다. 따라서 RAM은 주소를 알고 있으면 어떤 위치의 데이터든지 직접 액세스할 수 있어 "임의 액세스"가 가능합니다.
<br/>
<br/>

## SRAM(Static Random Access Memory)

cache(SRAM)은 cpu와 메인메모리(DRAM) 사이레벨에 있는 저장 장치입니다.

캐시 메모리라고 하면 실제 메모리와 CPU 사이에서 빠르게 전달을 위해서 미리 데이터들을 저장해두는 좀더 빠른 메모리입니다. 매번 메모리에 왔다갔다하면 시간이 많이 소요되므로 중간 장치를 두어 자주 사용하는 명령어를 가져다 놓는 것이지요.

cache는 명령어를 저장하는 icache와 데이터를 저장하는 dcache로 나눌 수 있습니다. 

icache는 명령어 (instruction) 캐시라고도 불리며, CPU에서 실행할 프로그램의 명령어를 저장하는 역할을 합니다. 명령어들은 주로 순차적으로 실행되므로, I-cache는 명령어의 spatial-locality를 활용하여 효율적으로 동작합니다. dcache는 데이터 캐시라고도 불리며, CPU에서 사용하는 데이터를 저장하는 역할을 합니다. 데이터의 액세스 패턴은 I-cache와 다르게 예측하기 어려울 수 있으므로, D-cache는 데이터의 temporal-locality를 활용하여 효율적으로 동작합니다.

cache는 또한 데이터를 저장하는 방식에 따라 여러가지로 나눌 수 있는데, 그 중 3가지를 살펴보도록 합시다.

### Directed mapped cache

Direct-mapped cache는 가장 간단한 형태의 캐시 구조입니다. 저장 될때는 index, tag, valid가 사용되는데, 메모리의 주소값에 따라 cache의 주소가 결정됩니다.

![Untitled 1](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/52e825cd-250c-4885-ac1d-c7a8bd589874)

예를 들어 11001이라는 값은 뒤의 3비트 001이 index-bit가 되고, 앞의 2비트 10이 tag-bit가 됩니다. 001의 index의 10의 자리에 저장하면 되는 것이죠

![Untitled 2](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/f904f266-5cf4-45ec-886b-a35639a01fb2)

그 다음 01011이 들어오면 이렇게 저장되겠군요.

![Untitled](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/47c7f9e3-083c-48c1-a7a2-1f95f6b35710)

이미 001 index에는 10이 존재하는데 00001이 들어오면 어떻게 될까요? 이 경우에는 10을 쫓아내고 00을 집어 넣습니다. 최근에 사용한 값이 또 사용될거라는 믿음이 있기 때문이죠.

이렇게 Direct-mapped 캐시의 장점은 구현이 간단하고 빠른 접근 시간을 제공하지만. 하지만 한 번에 한 개의 블록만 저장할 수 있으므로, 여러 주소가 동일한 인덱스를 가리키는 경우 충돌이 발생합니다. 충돌이 발생하면 기존의 캐시 블록을 대체하게 되는데, 이는 성능 저하를 야기할 수 있습니다.

또한 directed mapped cache에는 valid-bit라는게 존재합니다. valid-bits는 garbage 데이터 인지 확인하는 bit 입니다. 이는 우연히 우리가 찾는 데이터의 index와 tag비트가 같을 수 있기 때문이다. 따라서 컴퓨터 부팅시에 모든 valid를 0으로 초기화 하고 cache에 데이터가 올라 왔을때 유효한 데이터가 되므로 valid bit를 1로 바꾸어 줍니다. 이렇게 되면 cache에서 데이터를 가져갈때에는 valid bit가 1인것들만 가져갈 수 있게 됩니다..
<br/>
<br/>

### Associative cache

Associative cache는 direct-mapped cache의 단점인 충돌 문제를 완화하기 위해 개선된 캐시 구조입니다. 이 캐시는 여러 개의 집합(set)으로 구성되며, 각각의 집합은 여러 개의 라인(line)으로 나뉘어집니다. 각 집합은 direct-mapped cache에서 사용되는 인덱스의 일부를 사용하여 선택됩니다.

Set-associative cache는 충돌 문제를 완화하기 위해 여러 개의 라인을 사용하므로 충돌이 발생할 가능성이 줄어듭니다. 하지만 direct-mapped cache보다는 구현이 복잡해지고, 캐시의 용량이 증가합니다.

예를들어 cache size가 8인 2-way associative cache에 주소가 12인 데이터를 넣는다면

2-way이므로 8 / 2 = 4개의 block으로 구성되고 12 mod 4 = 0 이므로 0번 block의 두개중 하나에 들어가게 된다.

이때 만약에 새롭게 데이터가 들어왔는데 cache에 두자리가 모두 꽉차있다면

- LRU - 접근된지 가장 오래된 데이터를 쫓아내거나
- RANDOM - 그냥 랜덤으로 골라서 쫓아내는 방법이 있다.
<br/>
<br/>

### Fully Associative cache

Fully Associative cache는 그냥 cache의 빈공간에 아무곳에나 데이터를 집어 넣는 방식이다.

이는 캐시 라인을 원하는 위치에 저장할 수 있으므로 충돌 문제가 발생하지 않습니다. 하지만 이러한 유연성은 검색과 매핑을 위해 추가 하드웨어 논리가 필요하며, 더 높은 구현 비용과 지연 시간을 초래할 수 있습니다.

![Untitled 3](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/4c2ca475-91c1-4f27-bfa3-277921ebcfc7)
<br/>
<br/>

### cache line

Temporal locality(시간 지역성)은 데이터 또는 명령어에 대한 접근 패턴을 기반으로 합니다. 이는 한 번 접근한 데이터나 명령어에 대해 향후 일정 시간 동안 다시 접근할 확률이 높다는 개념입니다. 즉, 최근에 접근한 데이터 또는 명령어에는 잠시 후에 다시 접근할 가능성이 높다는 것을 의미합니다. 이러한 지역성은 데이터나 명령어의 재사용을 통해 캐시 메모리의 효율성을 높일 수 있습니다.

Spatial locality(공간 지역성)은 데이터 또는 명령어에 대한 접근 패턴을 기반으로 합니다. 이는 한 번 접근한 데이터나 명령어와 인접한 주변 데이터나 명령어에도 접근할 가능성이 높다는 개념입니다. 즉, 메모리에서 인접한 주소에 위치한 데이터나 명령어에도 접근할 가능성이 높다는 것을 의미합니다. 이러한 지역성은 캐시 메모리의 블록 단위로 데이터를 가져오는 것과 같이 공간적으로 인접한 데이터의 사본을 활용하여 캐시 히트율을 높일 수 있습니다.

위에서는 한개의 cache block이 한개의 데이터를 가지고 있다고 가정하였다. 하지만 이렇게 메모리에서 데이터를 한개 가져오는 것은 비효율적이다. 보통 메모리에서 캐시에 데이터를 가져올때는 spatial locality를 효율적으로 사용하기 위해서 여러개의 데이터를 한번에 가져온다.

cache line의 크기가 크면 spatial locality가 커서 miss rate는 줄어들지만, 

cache의 크기는 한정되어 있고 cache line의 크기가 커질 수록 저장할 수 있는 cache line들의 개수가 줄어들기 때문에 결국 miss rate가 올라가게 된다.

또한 크기를 키우면 pollution data(옆에 있지만 사용하지 않는 데이터)가 많아 질 수 있어 miss rate가 올라가게 된다.

마지막으로 cache line의 크기가 크면 메모리에서 데이터를 가져올때 한번에 많이 가져오기 떄문에 miss penalty도 그만큼 커진다.

따라서 적당한 cache line의 크기를 찾는 것이 중요하다.
<br/>
<br/>

### write hit

cache의 write hit은 쓰기 연산이 발생했을 때 캐시에 해당 데이터의 이전 상태가 이미 존재하여 이를 업데이트만 해주면 되는 상태를 의미한다. 이때 프로세서는 이 데이터를 cache에만 써줄 수 있고, cache에 쓴다음 메모리에 또 써줄 수 있다.

- Write-through: Write-through 캐시는 쓰기 연산이 발생하면 동시에 캐시와 메모리 양쪽에 데이터를 업데이트합니다. 따라서 Write Hit이 발생하더라도 해당 데이터는 캐시와 메모리 양쪽에 동시에 일관성 있게 업데이트됩니다.
- Write-back: Write-back 캐시는 쓰기 연산이 발생하면 캐시에만 데이터를 업데이트하고, 메모리는 나중에 업데이트하는 방식입니다. Write Hit이 발생하면 해당 데이터는 캐시에 업데이트되고,
    
    dirty bit가 1로 저장된다( cache에서의 데이터와 메모리에서 데이터가 다르다는 신호) 
    
    다른 데이터가 들어와 cache에서 쫓겨날때 dirty bit가 1이라면 그제서야 main memory에 적어준다.
<br/>
<br/>

### cache miss

캐시 미스(Cache miss)는 프로세서가 캐시 메모리에서 원하는 데이터나 명령어를 찾지 못하는 상황을 말합니다. 일반적으로 캐시는 빠른 액세스 시간과 작은 용량을 가지고 있어 CPU의 작업 속도를 향상시키는 역할을 합니다. 그러나 캐시 용량이 제한되어 있기 때문에 원하는 데이터나 명령어가 캐시에 없는 경우 캐시 미스가 발생합니다. 캐시 미스가 발생하면 해당 데이터나 명령어를 가져오기 위해 메인 메모리나 상위 수준의 캐시로 액세스해야 합니다.

여기서 Cache의 miss-penalty는 주로 사이클(cycle) 단위로 표현됩니다. CPU는 클럭 사이클을 기반으로 작동합니다. 클럭은 일정한 주기로 진동하며, 각 클럭 사이클은 CPU가 한 번의 연산 또는 동작을 수행할 수 있는 최소 시간 단위입니다. 따라서, miss-penalty를 클럭 사이클로 표현하면 해당 연산 또는 동작을 수행하는 데 걸리는 추가적인 시간을 정확히 측정할 수 있습니다.

일반적으로 메인 메모리에 액세스하는 시간은 캐시 액세스보다 훨씬 느립니다. 메인 메모리의 액세스 시간은 몇십에서 몇백 클럭 사이클이 소요될 수 있으며, 캐시 액세스는 보통 몇 클럭 사이클 이내에 완료됩니다. 따라서 캐시 미스가 발생할 때마다 해당 데이터를 메인 메모리에서 가져오는 시간은 상당한 지연을 일으킬 수 있습니다.

실제로 캐시 미스가 작업의 속도 저하를 얼마나 유발하는지는 다양한 요소에 따라 다릅니다. 하지만 단순히 계산으로 얼마나 느려지는지 알아봅시다.

I-cache의 miss rate는 2%, D-cache의 miss rate는 4%이다.  miss-penalty는 100cycle이고, load&store명령어는 instruction의 36%이다. Base CPI가 2일때 cache-miss가 전혀 일어나지 않는 프로세스보다 얼마나 느려지는가?

instruction개수를 I라고 가정하면,

instruction miss cycle = I x 2% x 100 = I x 2.0

data miss cycle = I x 36% x 4 % x 100 = I x 1.44

actual CPI = 2 + 2 + 1.44 = 5.44

따라서 5.44 / 2 = 2.72배

이 문제를 통해, miss rate가 cache당 각각 2%와 4%밖에 되지 않는데 속도는 3배 가까이 느려진 것을 알 수 있습니다. 이는 새로운 내용이 main-memory로 부터, 캐시 메모리에 복사되어야 하는 상황이여서 miss-panelty가 워낙 크기 때문입니다. 이를 통해 cache의 miss는 심각한 속도 저하를 일으킨다는 것을 확인할 수 있습니다.
<br/>
<br/>

### Multilevel cache

위에서 cache의 miss는 치명적이라는 것을 확인 할 수 있었습니다. 그렇다면 위와 같이 cache의 miss-rate를 줄이는 것이 우리에게 주어진 문제임을 알 수 있죠. 그렇다면 cache의 miss-rate는 어떻게 줄일 수 있을까요?

그건 cache를 여러 계층으로 나눈 multi-level cache를 이용하는 것입니다.
![Untitled 4](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/bf1e98c3-c079-45df-8a78-9ebc88d3989d)

![Untitled 5](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/2625cc88-32e6-45a8-9581-36583d0a4986)


위와 같이 가장 작고 가장 빠른 L1 cache, L1보다는 느리지만 조금 더 큰 L2, L3.. 이런 식으로 상위 계층 cache에서 miss가 난 data를 밑의 계층 cache에서 찾아내는 것입니다.
<br/>
<br/>

### 얼마나 빨라지는데?

위에서 한 것 처럼 계산을 통해 증명해보죠.

base CPI가 1이고 clock rate가 4GHZ인 컴퓨터라고 가정합시다.

L1 cache의 miss rate는 2%이고 main-memory의 접근 시간은 100ns입니다.

clock rate가 4GHZ이므로 한번 cycle이 왔다갔다하는데 걸리는 시간은 0.25ns입니다

main-memory의 접근시간이 100ns이라고 했으므로, cache에 데이터가 없어 main-memory에 가려면 

400cylce의 miss-penalty가 발생합니다.

그렇다면 effective CPI는 1 + 0.02 x 400 인 9라고 계산할  수 있습니다.

만약 L2 cache가 추가 된다면?

접근 시간이 5ns이고 miss-rate는 0.5% (L2에서도 찾는 데이터가 없을 확률)인 L2-cache가 있다고 하겠습니다. 그렇다면 effective CPI는 1 + 0.02 x 20 + 0.005 x 400인 3.4라고 계산할 수 있습니다.

L2 cache하나를 추가했을 뿐인데 무려 속도가 3배 빨라진 것을 확인할 수 있습니다. 

L1 cache는 보통 hit time을 줄이는데, 즉 데이터를 빠르게 찾아서 serving하는데에 초점을 맞추고 있기 때문에 매우 작게 만듭니다.

반면 L2 cache는 miss-rate를 줄이는데 초점이 맞추어져 있기 때문에 L1 cache보다 훨씬 사이즈가 큽니다.

보통 L1 cache은 32kb정도의 크기이고, L2 cache는 256KB에서 몇 메가바이트(MB) 사이입니다.
<br/>
<br/>

## SRAM vs DRAM

### 왜 Dynamic인가?

dynamic은 static과 반대로 변화하는 것을 의미합니다. DRAM은 커패시터(축전기) 방식으로 작동하는데. 커패시터(축전기)의 특징은 시간이 지나면 전하를 잃어 스스로 방전이 된다는 것입니다. 이는 시간이 지나면 조금씩 데이터가 변경된다는 소리이기에, DRAM에는 refresh회로가 추가로 달려있고 주기적으로 refresh 신호를 보내주어야합니다.

일반적으로 DRAM은 64ms 동안의 Refresh 주기를 가지며, 64ms의 주기를 가지는 경우, DRAM은 약 15.625회(1초를 64ms로 나눈 값)의 Refresh를 수행합니다. 따라서 약 15.625초에 한 번씩 데이터를 Refresh하게 됩니다.

Refresh 주기 동안에는 데이터 액세스가 이루어지지 않으며, DRAM은 Refresh 신호를 우선적으로 처리합니다. 이러한 Refresh 과정은 DRAM의 동작 속도에 영향을 미칠 수 있으며, Refresh 주기가 짧을수록 DRAM 성능이 더욱 향상됩니다. 그러나 Refresh 주기가 너무 짧으면 데이터의 유지가 어려워질 수 있으며, Refresh 주기를 적절히 설정하는 것은 DRAM의 정상적인 작동을 보장하는 중요한 요소입니다. 당연히 DRAM은 커패시터의 충방전을 계속해서 신경써야하므로 메모리만 기억하면 되는 SRAM보다 속도가 느리게 됩니다.

### 왜 **Static인가?**

static은 dynamic과 반대로 정적인, 변화하지 않는 것을 의미합니다. SRAM은 플리플롭(Flip-Flop)방식으로 작동합니다. 플리플롭은 전류신호가 오기 전에는 상태가 변화하지 않는 소자이기 때문에 가만히 놔두면 내용이 소멸, 변화하지 않는 안정적인 메모리 입니다. 하지만 당연히 휘발성 메모리이므로 전원을 끄면 데이터는 모두 날라갑니다.
<br/>
<br/>

# Physical Memory & Virtual Memory

## Physical Memory

앞서, 우리는 프로그램을 메모리에 적재할 때 메모리 내에 프로세스들이 연속적으로 배치되는 **연속 메모리 할당** 상황을 가정하였습니다. 프로그램 A에 대한 명령어들이 100번~200번의 주소를 차지하고 그 다음 프로그램 B에 대한 명령어들이 201번~200번의 주소를 차지하는 방식이죠. 이렇게 되면 무슨 문제가 나타날까요? 먼저 문제를 살펴보기전에 스와핑에 대해 알아봅시다.
<br/>
<br/>

### 스와핑

메모리에 적재된 프로세스들 중 입출력 작업으로 대기가 된 프로세스라던지, 오래동안 사용되지 않는 프로세스들을 잠시 보조기억장치의 영역으로 쫒아내고, 생긴 공간에 다른 프로세스를 적재하여 실행하는 방식을 **스와핑**이라고 합니다. 이때 프로세스가 쫓겨난 보조기억장치의 일부 영역을 **스왑영역**, 실행되지 않는 프로세스가 스왑영역으로 쫓겨자는 것을 **스왑아웃**, 스왑영역에 있던 프로세스가 다시 메모리로 돌아오는 것을 **스왑인**이라고 합니다.

![IMG_A4A77A8927F0-1](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/2d4f92ac-bd2a-4e85-aef7-c4f869e6cede)

위 그림처럼 **스와핑**을 이용하면 프로세스들이 요구하는 메모리 주소 공간의 크기가 실제 메모리 크기보다 큰 경우에도 프로세스들을 동시 실행할 수 있습니다.

### 메모리 공간에 프로세스를 할당하는 방식

- 최초 적합

최초 적합은 운영체제가 메모리 내의 빈 공간을 순서대로 검색하다가 적재할 수 있는 공간을 발견하면 그 공간에 프

로세스를 배치하는 방식입니다. 최초 적합 방식은 프로세스가 적재될 수 있는 공간을 발견하는 즉시 메모리를 할당하는 방식이므로 검색을 최소화하고 결과적으로 빠른할당이 가능합니다.

- 최적 적합

최적 적합은 운영체제가 빈 공간을 모두 검색한 후, 프로세스가 적재될 수 있는 공간 중 가장 작은 공간에 프로세스를 배치하는 방식입니다.

- 최악 적합

최적 적합은 운영체제가 빈 공간을 모두 검색한 후, 프로세스가 적재될 수 있는 공간 중 가장 큰 공간에 프로세스를 배치하는 방식입니다.
<br/>
<br/>

### 외부 단편화

![Untitled 2](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/2cf74aea-f24f-4ef2-a9c6-09a3ecfb5f84)

이렇게 프로세스를 연속적으로 메모리에 할당하는 것은 사실 메모리를 효율적으로 사용하는 방법이 아닙니다. 왜냐면 연속 메모리 할당은 외부단편화 문제를 내포하기 때문입니다. 외부 단편화란, 위 사진 처럼 프로세스들이 메모리에 연속적으로 할당되는 환경에서 프로세스들이 실행되고 종료하기를 반복하며 메모리 사이 사이에 빈 공간들이 생기게 되는데. 이 빈 공간들은 연속적이지 않고 빈 공간보다 큰 Process C 같은 프로그램은 적재하지 못하게 되면서 메모리 낭비로 이어지는 현상을 말합니다.
<br/>
<br/>

## Virtual Memory

위와 같이 프로세스를 메모리에 연속적으로 할당해야 한다면 메모리 보다 큰 프로그램은 실행할 수 없게되고 위에

서 말한 외부 단편화 문제가 발생합니다. 때문에 컴퓨터는 **가상메모리**라는 것을 사용합니다.

**가상 메모리**는 실행하고자 하는 프로그램을 일부만 메모리에 적재하여 실제 물리 메모리 크기보다 더 큰 프로세스

를 실행할 수 있게하는 기술입니다. 이 가상 메모리는 **페이징** **기법을** 통해 관리됩니다.

### 페이징 기법

연속 메모리 할당 방식에서 외부 단편화가 발생한 이유는 **각기 다른 크기의 프로세스가 메모리에 연속적으로 할당**

**되었기 때문이었습니다.** 만일 프로세스를 일정한 단위로 자르고, 이를 메모리에 불연속적으로 할당할 수 있다면 외부 단편화는 발생하지 않을 것입니다. 이렇게 프로세스를 **페이지**라는 일정한 단위로 자르고, 메모리를 똑같은 크기인 **프레임**이라는 단위로 자른뒤 **페이지**를 **프레임**에 할당하는 것이 **페이징 기법입니다**.

페이징 기법에서도 위에서 언급한 **스와핑**을 사용할 수 있습니다. 대신 페이징 기법에서는 프로세스 전체가 스왑 

인/아웃되지 않고 페이지 단위로 인/아웃 됩니다. 이걸 **페이지 아웃/ 페이지 인**이라합니다.
<br/>
<br/>

### 페이지 테이블

![IMG_C90EE61AC389-1](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/fd4fdaf1-0625-4f0a-b6ad-3eb2f99fbcaa)

하지만 이렇게 페이지 기법에서 페이지 단위로 잘라서 메모리에 불연속적으로 배치하면 cpu 입장에서는 이를 순차적으로 실행할 수 없게 됩니다. cpu가 하나의 프로세스를 이루는 페이지가 어떤 프레임에 적재되어있는지 모두 알수는 없기 때문입니다.

이를 해결하기 위해 프로세스가 비록 물리 주소에 불연속적으로 배치되더라도, 논리 주소에는 연속적으로 배치되

도록하는 페이지 테이블을 이용합니다. 페이지 테이블은 페이지 번호와 프레임 번호를 짝지어 주어 cpu로 하여금 페이지 번호만 보고 해당 페이지가 적재된 프레임을 찾을 수 있게 합니다.

이렇게 하면 메모리에서는 프로세스들이 분산되어 저장되어 있더라도 cpu 입장에서 바라보는 논리 주소는 연속적

으로 보이게 할 수 있게 됩니다. 프로세스 마다 각자의 페이지 테이블을 가지고 있고, 각 프로세스의 페이지 테이블은 메모리에 적재되어 있습니다. 

![Untitled 3](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/2f4d0512-57c0-45a9-8bb8-eb721b0d82b9)

이제 pagetable이 추가되었을때 cpu가 명령어를 처리하는 과정을 살펴볼까요?

다음 명령어의 주소가 저장된 프로그램 카운터 레지스터에 저장된 값으로 cache에 접근을 먼저합니다. 하지만 cache에 해당 주소에 대한 명령어나 값이 존재하지 않으면 페이지 테이블 레지스터에 저장된 페이지 테이블의 주소를 들고 메모리에가서 페이지 테이블을 찾아냅니다. 거기서 가상주소를 진짜 주소로 바꾸고 다시 메모리에서 해당 주소의 명령어나 값을 가져와서 명령어 레지스터에 저장한다음 cpu는 해당 명령을 수행하게 됩니다. 
<br/>
<br/>

### Page fault

page fault는 페이지 테이블에서 진짜 주소를 찾아냈는데, 이 페이지가 메모리에 존재하지 않고 외부저장장치에 있을때 발생합니다. 정말 최악의 상황이죠. 메모리에 갔다오는 것도 수많은 시간이 소요되어 cache라는 것을 만들었는데 메모리보다 먼 외부 저장 장치에 갔다오는 시간은 수백만 clock cycle을 소요하므로, 프로그램의 속도에 엄청난 영향을 미칩니다. 따라서 
<br/>
<br/>

### multi-level 페이지 테이블

메모리 공간이 커질수록 단일 수준의 페이지 테이블을 사용하면 페이지 테이블의 크기가 커지는 문제가 발생합니다. 예를 들어, 32비트 아키텍처에서 4KB 크기의 페이지를 사용하고 있다면, 전체 가상 주소 공간은 4GB가 됩니다. 이 경우, 단일 수준의 페이지 테이블은 4GB의 페이지 테이블 엔트리로 구성되어야 합니다. 이는 메모리 낭비와 검색 시간 증가로 이어질 수 있습니다.

멀티레벨 페이지 테이블은 이러한 문제를 해결하기 위해 사용됩니다. 페이지 테이블을 여러 레벨로 구성함으로써 페이지 테이블의 크기를 줄일 수 있습니다. 높은 레벨의 페이지 테이블은 저수준 페이지 테이블 엔트리들을 가리키고, 이 저수준 페이지 테이블 엔트리들은 실제 페이지를 가리킵니다. 이를 통해 전체 페이지 테이블의 크기를 줄이고, 가상 주소 변환에 필요한 검색 시간을 단축할 수 있습니다.
<br/>
<br/>

### TLB

근데 이상하지 않나요? 우리는 메모리에 접근을 최소화 하기위해 cache란 것을 만들었습니다. 그리고 cache miss가 났을때 얼마나 큰 cycle이 낭비되는지 계산으로 확인해 보았습니다. 그런데 페이지 테이블을 찾기 위해 메모리에 접근한다니 이건 너무 모순되는 말입니다. 또한 페이지 테이블을 메모리에서 찾고 매핑하여 진짜 주소를 찾아낸 다음 메모리를 또 접근해야합니다. 메모리를 두번 접근하는 것은 엄청난 시간 낭비를 초래합니다.

따라서 이와 같은 시간 낭비를 해결하기 위해 cpu곁에 **TLB**라는 페이지 테이블의 캐시 메모리를 둡니다. 자주 사용하는 프레임의 주소를 페이지 주소와 짝지어 저장하는 것이죠. 

이는 page는 단위별로 묶여져 있기 때문에 locality가 좋기 때문에 가능한 일입니다. cpu가 발생한 논리 주소에 대한 페이지 번호가 **TLB**에 있을 경우 **TLB 히트**라고 하고 이때는 메모리에 한번만 접 근하면 됩니다. 하지만 페이지 번호가 TLB에 없을 경우 **TLB 미스**라고 하고 프레임 주소를 알기 위해 위와같이 메모리 내의 페이지 테이블에 접근해야 합니다.

![Untitled 4](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/8352aba0-a009-4b08-b232-432d365b0f29)
<br/>
<br/>

### 내부 단편화
페이징은 외부 단편화 문제를 해결할 수 있지만, 내부 단편화 문제가 생길 수 있습니다. 페이징 기법은 프로세스를 일정한 크기 단위로 자른다 하였는데 모든 프로세스가 페이지 크기의 배수가 아니므로 정확히 잘리는 것은 아닙니다. 이 경우 자르고 남는 부분이 생길 수 있다 이러한 메모리 낭비를 내부 단편화라고 합니다.
<br/>
<br/>

### **페이징에서의 주소 변환**

![Untitled 5](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/86ddf6e8-32b1-4039-b011-a2805c9143e2)

페이징 기법에서 특정 주소에 접근하려면 **어떤 페이지에 접근할 것인지**와 **접근하려는 주소가 그 페이지로부터 얼마나 떨어져 있는지** 두가지 정보가 필요하다.

![Untitled 6](https://github.com/gkqkehs7/Computer-Structure/assets/77993709/4533a961-56d1-4f8e-ab4c-0b7a99ad684f)

따라서 페이징 시스템에서는 모든 논리 주소가 기본적으로 **페이지 번호**와 **변위**로 되어있다. 

변위는 접근하려는 주소가 프레임의 시작 번지로부터 얼만큼 떨어져 있는기 알기 위한 정보이다.

즉, 논리주소 **<페이지번호, 변위>**는 페이지 테이블을 통해 **<프레임번호, 변위>**로 변환된다.
<br/>
<br/>

### 페이지 테이블 엔트리

페이지 테이블에는 사실 페이지 번호, 프레임 번호만 있는 것이 아니라 다른 중요한 정보들이 있다.

- 유효비트
    
    유효비트는 현재 해당 페이지에 접근 가능한지 여부를 알려준다. 위에서 프로세스는 스와핑을 통해 보조기억장치와 메모리를 왔다갔다 한다고 하였다. 프로세스는 사실 대부분 보조기억장치(스왑 영역)에 있는 경우가 많다. 유효 비트는 현재 페이지가 메모리에 적재되어 있는지 스왑영역에 있는지 알려준다.
    
    만일 cpu가 스왑영역에 있는 페이지로 접근하려 하면 **페이지 폴트**가 발생하고, 해당 페이지를 메모리로 가져온뒤 접근한다.
    
- 보호비트
    
    보호비트를 통해 페이지가 읽고 쓰기가 모두 가능한 페이지인지, 혹은 읽기만 가능한 페이지 인지 나타낼 수 있다.
    
- 참조비트
    
    참조비트는 cpu가 페이지에 접근한 적이 있는지 여부를 나타낸다.
    
- 수정비트
    
    수정비트는 해당 페이지에 데이터를 쓴 적이 있는지 없는지 수정 여부를 나타낸다. 수정 비트는 페이지가 메모리에서 사라질때 보조기억장치에 쓰기 작업을 해야하는지, 할 필요가 없는지 판단하기 위해 존재하
<br/>
<br/> 

### **쓰기 시 복사**

프로세스를 fork하여 동일한 두 개가 복제되면 모든 자원이 복제되어 메모리에 적재된다 하였다. 부모 프로세스가 

자식프로세스를 생성하면 새롭게 생성된 자식 프로세스의 코드 및 데이터 영역은 부모 프로세스의 메모리 영역과 

다른 새로운 공간에 할당된다. 이것은 불필요한 메모리 낭비를 야기한다. 반면 페이징 기법을 사용하면 부모 프로

세스와 자식프로세스가 동일한 프레임을 가리킨다. 따라서 부모 프로세스의 메모리 공간을 복사하지 않고도 동일

한 코드 및 데이터 영역을 가리킬 수 있다. 

하지만 프로세스 간에는 자원을 공유하지 않으므로 둘 중 하나의 프로세스에서 쓰기 작업을 하면 페이지가 별도의 

공간으로 복제된다.
<br/>
<br/>

### 계층적 페이징

프로세스의 크기가 커지면 프로세스의 테이블 크기도 커지기 때문에 모든 페이지의 테이블 엔트리를 메모리에 두

는 것은 메모리 낭비이다. 따라서 페이지 테이블을 다시 페이징하여 여러단계의 페이지를 두는 방식을 **계층적 페이**

**징**이라고 한다.
<br/>
<br/>

## 페이지 테이블 관리하기

가상 메모리를 통해 물리 메모리보다 큰 프로세스도 실행할 수 있다고 하지만, 그럼에도 물리 메모리의 크기는 한정되어있다. 운영체제는 이를 어떻게 관리하고 해결할까?
<br/>
<br/>

### 요구 페이징

프로세스를 메모리에 적재할때 처음부터 모든 페이지를 적재하지 않고, 필요한 페이지만을 메모리에 적재하는 기법을 **요구 페이징**이라고 한다. 그렇지만 요구 페이징 기법으로 페이지들을 적재하다 보면 언젠가 메모리가 가득 차게 된다. 이때는 당장 실행에 필요하지 않는 메모리에 적재된 페이지를 보조기억장치로 내보내야 한다. 이렇게 쫓아낼 페이지를 결정하는 방법을 **페이지 교체 알고리즘**이라고 한다. **페이지 폴트**가 일어나면 보조 기억장치로부터 필요한 페이지를 가져와야 하기 때문에 **페이지 폴트**를 가장 적에 일으키는 알고리즘이 좋은 알고리즘으로 평가된다.
<br/>
<br/>

### 페이지 교체 알고리즘의 종류

- FIFO 알고리즘
    
    메모리에 가장 먼저 올라온 페이지부터 내쫓는 방식이다. 오래 있었다고 쫓아 내기엔 프로그램 실행 내내 사용될 내용을 포함할 수도 있기 때문에 좋은 알고리즘은 아니다.
    
- 최적 페이지 교체 알고리즘
    
    cpu에 의해 참조 되는 횟수를 고려하는 페이지 교체 알고리즘이다. 사용 빈도가 가장 낮은 페이지를 보조기억 
    
    장치로 내쫓는 것이다. **최적 페이지 교체 알고리즘**은 가장 낮은 페이지 폴트율을 보장하는 알고리즘이다. 다만
    
    앞으로도 오랫동안 사용되지 않을 페이지를 예측하기는 어렵기 떄문에 실제 구현이 어렵다. 따라서 **최적 페이**
    
    **지 교체 알고리즘**은 실제 운영체제에서 사용하기 보다는, 다른 페이지 교체 알고리즘의 성능을 평가하기 위한 
    
    목적으로 사용된다.
    
- LRU 페이지 교체 알고리즘
    
    LRU페이지 교체 알고리즘은 페이지마다 마지막으로 사용한 시간을 토대로 최근에 가장 사용이 적었던 페이지
    
    를 내쫓는다.

<br/>
<br/>   

### 스래싱

페이지 폴트는 프로세스가 사용할 수 있는 프레임수가 적을때 더 자주 발생한다. 당연히 프레임이 부족하면 cpu는 

페이지 폴트가 자주 발생할 수밖에 없다. 이렇게 프로세스가 실제 실행되는 시간보다 페이징에 더 많은 시간을 소요

하여 성능이 저해되는 문제를 스래싱이라고 한다.

동시에 실행되는 프로세스 수가 어느정도 하면 cpu이용률이 높아지지만, 필요 이상으로 늘리면 각 프로세스들이 

사용할 수 있는 프레임 수가 적어지기 때문에 페이지 폴트가 빈번히 발생하고, 이는 cpu 이용률의 저하를 야기한

다. 따라서 **운영체제는 프로세스들이 무리없이 실행하기 위한 최소한의 프레임 수를 파악하고 프로세스들에게 적**

**절한 수만큼 프레임을 할당해야 한다.**

### 프레임할당

- 균등할당
    
    각 프로세스에 똑같은 개수만큼 프레임을 할당하는 방법이다. 이는 크기가 큰 프로세스와 작은 프로세스가 같은 개수의 프레임을 할당받야하 하므로 비효율적이다.
    
- 비례할당
    
    크키가 큰 프로세스에겐 많은 프레임을, 크기가 작은 프로세스에겐 적은 프레임을 할당하는 방식이다. 하지만 프로세스의 크기가 작아도 실행해보니 프레임이 많이 필요한 경우가 있기 때문에 이도 비효율적이다.
    
- 페이지 폴트 빈도 기반 프레임 할당 방식
    
    cpu가 특정시간 동안 주로 참조한 페이지 개수만큼만 프레임을 할당한다.
    
    프로세스가 일정 시간동안 참조한 페이지의 집합을 참조 집합이라고 한다. 운영체제는 프로세스의 참조 집합의 개수가 10초 당안 20개였다면 20개의 프레임을 할당한다. 그 다음 페이지 폴트율의 상한선과 하한선을 정하고만약 페이지 폴트 비율이 상한선보다 높아지면 너무 적은 프레임을 가지고 있으므로 더 많은 프레임을 할당하고, 페이지 폴트율이 하한선보다 낮아지면 다른 프로세스에게 할당하기 위해 프레임을 회수한다.